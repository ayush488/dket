"""Metrics for `dket` model evaluation."""

import itertools

import tensorflow as tf


class Metrics(object):
    """A function used to judge performances of the model."""

    def __init__(self, metrics):
        """Initialize a new instance.

        Arguments:
          metrics: a list of callable objects implementing the
            same signature of the `Metric.compute` method.
        """
        self._metrics = metrics

    def __call__(self, target, output, weights=None):
        """Wrapper for the `compute` method."""
        return self.compute(target, output, weights=weights)

    def compute(self, target, output, weights=None):
        """Compute a set of evaluation metrics on the results.

        Arguments:
          target: the truth label `Tensor`, with `tf.int32` as `dtype`. It has rank
            `[d_0, d_1, ..., d_{r-1}]` and the last value is supposed to range between
            `0` and `num_classes - 1`.
          output: the model output `Tensor` with `tf.float32` as `dtype`. It has shape
            `[d_0, d_1, ..., d_{r-1}, num_classes]` and dtype `float32` and represents the
            probability distribution across the output classes generated by the model.
          weights: coefficients for the loss. This must be scalar or of same rank as `labels`.

        Returns:
          a `list` of `Op` objects representing the evaluation metrics for the model.
        """
        ops = [metric(target, output, weights=weights) for metric in self._metrics]
        return list(itertools.chain(*ops))

    @staticmethod
    def mean_categorical_accuracy():
        """Compute the mean categorical accuracy on a batch.

        Returns:
          a `Metrics` instance that returns a list with one `Op` representing
          the average accuracy of the predicted output w.r.t. the target.
        """
        def _func(target, output, weights=None):
            return [mean_categorical_accuracy(target, output, weights=weights)]
        return Metrics([_func])


def mean_categorical_accuracy(target, output, weights=None):
    """Computes the mean categorical accuracy between `truth` and `predictions`.

    Arguments:
      target: the truth label `Tensor`, with `tf.int32` as `dtype`. It has rank
        `[d_0, d_1, ..., d_{r-1}]` and the last value is supposed to range between
        `0` and `num_classes - 1`.
      output: the model output `Tensor` with `tf.float32` as `dtype`. It has shape
        `[d_0, d_1, ..., d_{r-1}, num_classes]` and dtype `float32` and represents the
        probability distribution across the output classes generated by the model.
      weights: coefficients for the loss. This must be scalar or of same rank as `labels`.
      scope: `str` or `tr.VariableScope`, is the scope for the operations
        performed in computing the loss.

    Returns:
      A `Tensor` of `0D` (i.e. a scalar) representing the mean categorical accuracy.
    """
    actual = tf.cast(tf.argmax(output, axis=-1), tf.int32)
    is_equal = tf.cast(tf.equal(target, actual), tf.float32)
    if weights is not None:
        actual = tf.multiply(weights, is_equal)
        weights = tf.multiply(weights, tf.ones_like(is_equal))
        return tf.div(tf.reduce_sum(actual), tf.reduce_sum(weights))
    return tf.reduce_mean(is_equal)
