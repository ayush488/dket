#!/usr/bin/env python

import argparse
import copy
import json
import logging
import os
import shutil
import subprocess

from dket import configurable
from dket import model
from dket import train
from dket import runtime

CFG = runtime.Experiment.CONFIG_FILE_NAME
CLZ_MODEL_DEFAULT = 'PointingSoftmaxModel'
CLZ_OPTIMIZER_DEFAULT = 'SGD'

# pylint: disable=C0103,C0301,I0011
parser = argparse.ArgumentParser(prog='dket-create-experiment')
parser.add_argument('--name', type=str, help='the experiment name. If not set, the base directory name will be used.')
parser.add_argument('--logdir', type=str, required=True, help='The base directory for the new experiment to be created. If not existing, the directory will be created. If existing it MUST be empty. A config.json file with the experimental settings will be created.')
parser.add_argument('--force', action='store_true', help='If true, erease an already existing [LOGDIR]')
parser.add_argument('--clone-from', type=str, help='Another experiment directory to stem from. The config.json settings file will be cloned for further modification.')
parser.add_argument('--editor', type=str, help='An editor to be used to edit the newly created config.json file. If empty, no editor will be launched.')
parser.add_argument('--model', type=str, help='The class name of the model to be used.')
parser.add_argument('--optimizer', type=str, help='The class name of the optimizer to be used.')
parser.add_argument('--lr-decay', type=str, help='The class name of the learning rate decay function to be used.')
parser.add_argument('--grad-clip', type=str, help='The class name of the gradient clipping function to be used.')
# pylint: enable=C0103,C0301,I0011
ARGS = parser.parse_args()


def _prepare():
    logdir = os.path.abspath(ARGS.logdir)
    if os.path.exists(logdir):
        if os.listdir(logdir):
            if not ARGS.force:
                raise FileExistsError(
                    'The directory {} already exists and it is not empty.'.format(logdir))
        logging.warning('Directory {} already exists: removing and recreating.'.format(logdir))
        shutil.rmtree(logdir)
    os.makedirs(logdir)
    fpath = os.path.join(logdir, CFG)
    return logdir, fpath


def main():
    """Create the new experiment."""
    logdir, fpath = _prepare()
    if ARGS.clone_from:
        reference = json.load(open(os.path.join(ARGS.clone_from, CFG)))
        config = clone(
            reference=reference,
            logdir=logdir,
            name=ARGS.name or logdir,
            model_clz=ARGS.model,
            optimizer_clz=ARGS.optimizer,
            lr_decay_clz=ARGS.lr_decay,
            grad_clip_clz=ARGS.grad_clip)
    else:
        config = from_scratch(
            logdir=logdir,
            name=ARGS.name or logdir,
            model_clz=ARGS.model,
            optimizer_clz=ARGS.optimizer,
            lr_decay_clz=ARGS.lr_decay,
            grad_clip_clz=ARGS.grad_clip)

    json.dump(config, open(fpath, mode='w'), indent=4, separators=(',', ': '))
    return fpath


def clone(reference, logdir, name,
          model_clz=None, optimizer_clz=None,
          lr_decay_clz=None, grad_clip_clz=None):
    """Clone an existing configuration."""

    t_model = configurable.resolve(model_clz, model) if model_clz else None
    t_optimizer = configurable.resolve(optimizer_clz, train) if optimizer_clz else None
    t_lr_decay = configurable.resolve(lr_decay_clz, train) if lr_decay_clz else None
    t_grad_clip = configurable.resolve(grad_clip_clz, train) if grad_clip_clz else None

    result = copy.deepcopy(reference)
    result[runtime.Experiment.NAME_KEY] = name
    result[runtime.Experiment.LOGDIR_KEY] = logdir
    rparams = result[runtime.Experiment.PARAMS_KEY]
    if not rparams:
        t_model = t_model or configurable.resolve(CLZ_MODEL_DEFAULT, model)
        result[runtime.Experiment.PARAMS_KEY] = t_model.get_default_params()
        rparams = result[runtime.Experiment.PARAMS_KEY]
        
    
    if t_optimizer:
        rparams[model.Model.OPTIMIZER_CLASS_PK] = optimizer_clz
        rparams[model.Model.OPTIMIZER_PARAMS_PK] = t_optimizer.get_default_params()
    opt_params = rparams[model.Model.OPTIMIZER_PARAMS_PK]

    if t_lr_decay:
        opt_params[train.Optimizer.LR_DECAY_CLASS_PK] = lr_decay_clz
        opt_params[train.Optimizer.LR_DECAY_PARAMS_PK] = t_lr_decay.get_default_params()
    
    if t_grad_clip:
        opt_params[train.Optimizer.CLIP_GRADS_CLASS_PK] = grad_clip_clz
        opt_params[train.Optimizer.CLIP_GRADS_PARAMS_PK] = t_grad_clip.get_default_params()
    return result
    

def from_scratch(logdir, name,
                 model_clz=None, optimizer_clz=None,
                 lr_decay_clz=None, grad_clip_clz=None):
    """Build a default configuration experiment."""

    config = runtime.Experiment.get_default_config()
    config[runtime.Experiment.LOGDIR_KEY] = logdir
    config[runtime.Experiment.NAME_KEY] = name

    model_clz = model_clz if model_clz else CLZ_MODEL_DEFAULT
    t_model = configurable.resolve(model_clz, model)
    params = t_model.get_default_params()

    opt_class_key = model.Model.OPTIMIZER_CLASS_PK
    opt_params_key = model.Model.OPTIMIZER_PARAMS_PK
    optimizer_clz = optimizer_clz if optimizer_clz else CLZ_OPTIMIZER_DEFAULT
    t_optimizer = configurable.resolve(optimizer_clz, train)
    params[opt_class_key] = optimizer_clz
    params[opt_params_key] = t_optimizer.get_default_params()

    if grad_clip_clz:
        grad_clip_class_key = train.Optimizer.CLIP_GRADS_CLASS_PK
        grad_clip_params_key = train.Optimizer.CLIP_GRADS_PARAMS_PK
        t_grad_clip = configurable.resolve(grad_clip_clz, train)
        params[opt_params_key][grad_clip_class_key] = grad_clip_clz
        params[opt_params_key][grad_clip_params_key] = t_grad_clip.get_default_params()

    if lr_decay_clz:
        lr_decay_class_key = train.Optimizer.LR_DECAY_CLASS_PK
        lr_decay_params_key = train.Optimizer.LR_DECAY_PARAMS_PK
        t_lr_decay = configurable.resolve(lr_decay_clz, train)
        params[opt_params_key][lr_decay_class_key] = lr_decay_clz
        params[opt_params_key][lr_decay_params_key] = t_lr_decay.get_default_params()
 
    config[runtime.Experiment.PARAMS_KEY] = params
    return config


if __name__ == '__main__':
    FPATH = main()
    if ARGS.editor:
        subprocess.call([ARGS.editor, FPATH])
